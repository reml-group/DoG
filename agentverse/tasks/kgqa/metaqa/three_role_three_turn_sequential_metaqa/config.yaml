prompts:
  prompt: &prompt |
    The triplet [e, r, e'] is used to answer a sub-question of the multi-hop problem, resulting in a smaller question containing the answer entity (e or e') which is not in the original question. 
    The formal process of this type of task is:
      Given an N-hop problem Q_N;
      The target entities involved in Q_N are e_target and relationship r_target(containing sub-problems :(e_target,r_target, ? ),  and known information K = {(e_i,r_i,e_j)},
      Find triples that satisfy the following conditions:(e_target,r_target,e_know) or (e_know,r_target,e_target) belongs to K.
      Replace e_targt in Q_N with e_known to get Q_N-1.
    For exmaple:
        Q_N: what were the release years the films written by [Pierre Geller]?
        K: ['(Rebellion,written_by,Pierre Geller)']
        Q_N-1: What were the release years of the film [Rebellion]?
    Here is your task:
    [N-hop question]
    ${source_text}
    [known information]
    ${compared_text_one}
    ${role_description}
    Here's your discussion history:
    ${chat_history}
    ${final_prompt}

environment:
  env_type: llm_eval
  max_turns: 9
  rule:
    order:
      type: sequential
    visibility:
      type: all
    selector:
      type: basic
    updater:
      type: basic
    describer:
      type: basic

agents:
  -
    agent_type: llm_eval_multi
    name: Question Simplifying Expert
    final_prompt_to_use: |-

    role_description: |
      You are an expert in problem simplification. What you are good at doing is answering sub-questions of the original N-hop problem(Q_N) based on known information(K), thereby obtaining new problems(Q_N-1) that need to be solved.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-3.5-turbo-0125"
      llm_type: gpt-3.5-turbo
      temperature: 0
      max_tokens: 512
  -
    agent_type: llm_eval_multi
    name: Critic
    final_prompt_to_use: |-

    role_description: |
      You are a serious critic, please note: you need to compare [N-hop Question] and the [simplified_question] obtained from the discussion in the chat history to see if they are the same. If they are the same, it means that the previous expert problem simplification failed and the task was not completed. You need to point this error out. and give the answer you think is correct. Make sure that the simplified question includes e_known.
      Double check error cases where simplification was not successful !!
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-3.5-turbo-0125"
      llm_type: gpt-3.5-turbo
      temperature: 0
      max_tokens: 512
  -
    agent_type: llm_eval_multi
    name: Linguist
    final_prompt_to_use: |-
      You just need to output the correct simplified problem. Please be careful not to print the original question or duplicate a new question. The format of your output must be "simplified question:...."
    role_description: |      
      You are a linguist who is particularly good at dealing with irrelevant information in simplified problems. Regarding the simplified problems output by collaborators in the chat log, if the simplification is incomplete, please provide a reasonable simplified solution. In order to do your job better, learn how to avoid the mistakes of incomplete simplification.
      Specifically, you need to make sure that there are no e_target and r_target in the simplified problem Q_N-1, only e_known.
      Ensure that redundant entities and relationships do not reappear in the simplified problem !!
      That is, remove the attributive modification of known entities!
      For example:
        Q_N: what were the release years the films written by [Pierre Geller]?
        K: ['(Rebellion,written_by,Pierre Geller)']
        Q_N-1: What were the release years of the film [Rebellion] written by [Pierre Geller]? (Contains redundant information: "written by [Pierre Geller]")
        Correct Q_N-1: What were the release years of the film [Rebellion]?
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-3.5-turbo-0125"
      llm_type: gpt-3.5-turbo
      temperature: 0
      max_tokens: 512

tools: ~


